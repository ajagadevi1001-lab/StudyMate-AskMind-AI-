# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ndew2t0yN9zB523RYcX3gYpcPDTP2ML
"""

# ---------------------------
# Install dependencies
# ---------------------------
!pip install -q gradio pdfplumber sentence-transformers faiss-cpu huggingface_hub

import os
import time
import pdfplumber
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from huggingface_hub import InferenceApi
import gradio as gr

# ---------------------------
# Configuration
# ---------------------------
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 800
CHUNK_OVERLAP = 200
TOP_K = 4

SYSTEM_PROMPT = """
You are StudyMate, an academic assistant. Use ONLY the provided PDF passages to answer.
If answer not found, say: "The provided PDFs do not contain information about this topic."
Give concise, student-friendly explanations with citations.
"""

embedder = SentenceTransformer(EMBED_MODEL)


# ---------------------------
# PDF PROCESSING
# ---------------------------
def extract_pages(pdf_files):
    pages = []
    for name, file_bytes in pdf_files:
        temp_path = f"/tmp/{os.path.basename(name)}"
        with open(temp_path, "wb") as f:
            f.write(file_bytes)

        try:
            with pdfplumber.open(temp_path) as pdf:
                for pg, page in enumerate(pdf.pages, start=1):
                    text = page.extract_text() or ""
                    if text.strip():
                        pages.append({"file": name, "page": pg, "text": text.strip()})
        except Exception as e:
            print("Error reading:", name, e)
    return pages


def chunk_text(pages):
    chunks = []
    for p in pages:
        t = p["text"]

        if len(t) <= CHUNK_SIZE:
            chunks.append({"text": t, "file": p["file"], "page": p["page"]})
            continue

        start = 0
        while start < len(t):
            end = start + CHUNK_SIZE
            chunk = t[start:end]
            chunks.append({"text": chunk, "file": p["file"], "page": p["page"]})
            start = end - CHUNK_OVERLAP
    return chunks


# ---------------------------
# FAISS INDEX
# ---------------------------
def build_index(chunks):
    texts = [c["text"] for c in chunks]
    vectors = embedder.encode(texts, convert_to_numpy=True)
    faiss.normalize_L2(vectors)

    index = faiss.IndexFlatIP(vectors.shape[1])
    index.add(vectors)
    return index


def retrieve(query, chunks, index, k=TOP_K):
    qvec = embedder.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(qvec)
    D, I = index.search(qvec, k)

    results = []
    for idx, score in zip(I[0], D[0]):
        results.append((chunks[idx], float(score)))
    return results


# ---------------------------
# HF INFERENCE CALL
# ---------------------------
def call_granite(prompt, token, model="ibm-granite/granite-3.3-2b-instruct"):
    api = InferenceApi(repo_id=model, token=token)
    payload = {
        "inputs": prompt,
        "parameters": {"max_new_tokens": 500, "temperature": 0.0},
    }
    out = api(payload)

    if isinstance(out, list) and "generated_text" in out[0]:
        return out[0]["generated_text"]
    return str(out)


# ---------------------------
# BUILD PROMPT
# ---------------------------
def bui

# -------------------------------------------
# Install Dependencies
# -------------------------------------------
!pip install -q gradio pdfplumber sentence-transformers faiss-cpu huggingface_hub

import os
import time
import pdfplumber
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from huggingface_hub import InferenceApi
import gradio as gr


# -------------------------------------------
# Configuration
# -------------------------------------------
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 800
CHUNK_OVERLAP = 200
TOP_K = 4

SYSTEM_PROMPT = """
You are StudyMate, an academic assistant.
Use ONLY the provided PDF passages to answer questions.
If the answer is not found, say:
"The provided PDFs do not contain information about this topic."
Give concise explanations with page citations.
"""

embedder = SentenceTransformer(EMBED_MODEL)


# -------------------------------------------
# PDF Processing
# -------------------------------------------
def extract_pages(pdf_files):
    pages = []
    for name, file_bytes in pdf_files:
        temp_path = f"/tmp/{os.path.basename(name)}"

        with open(temp_path, "wb") as f:
            f.write(file_bytes)

        try:
            with pdfplumber.open(temp_path) as pdf:
                for pg, page in enumerate(pdf.pages, start=1):
                    text = page.extract_text() or ""
                    text = text.strip()
                    if text:
                        pages.append({"file": name, "page": pg, "text": text})
        except Exception as e:
            print("PDF read error:", e)

    return pages


def chunk_text(pages):
    chunks = []
    for p in pages:
        text = p["text"]
        if len(text) <= CHUNK_SIZE:
            chunks.append({"text": text, "file": p["file"], "page": p["page"]})
            continue

        start = 0
        while start < len(text):
            end = start + CHUNK_SIZE
            chunk = text[start:end]
            chunks.append(
                {"text": chunk, "file": p["file"], "page": p["page"]}
            )
            start = end - CHUNK_OVERLAP

    return chunks


# -------------------------------------------
# FAISS Index
# -------------------------------------------
def build_index(chunks):
    texts = [c["text"] for c in chunks]
    vectors = embedder.encode(texts, convert_to_numpy=True)

    faiss.normalize_L2(vectors)

    index = faiss.IndexFlatIP(vectors.shape[1])
    index.add(vectors)

    return index


def retrieve(query, chunks, index, k=TOP_K):
    qvec = embedder.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(qvec)

    D, I = index.search(qvec, k)
    results = []

    for idx, score in zip(I[0], D[0]):
        results.append((chunks[idx], float(score)))

    return results


# -------------------------------------------
# HuggingFace Granite Model Call
# -------------------------------------------
def call_granite(prompt, token, model="ibm-granite/granite-3.3-2b-instruct"):
    api = InferenceApi(repo_id=model, token=token)
    payload = {
        "inputs": prompt,
        "parameters": {"max_new_tokens": 400, "temperature": 0.0},
    }

    output = api(payload)

    if isinstance(output, list) and "generated_text" in output[0]:
        return output[0]["generated_text"]

    return str(output)


# -------------------------------------------
# Prompt Builder
# -------------------------------------------
def build_prompt(question, retrieved):
    ctx = ""

    for i, (c, score) in enumerate(retrieved, start=1):
        ctx += (
            f"\nPASSAGE {i} [file: {c['file']} | page: {c['page']}]:\n"
            f"{c['text']}\n"
        )

    prompt = (
        SYSTEM_PROMPT
        + "\n\nUse the passages below:\n"
        + ctx
        + f"\nQuestion: {question}\n\nAnswer with citations:"
    )

    return prompt


# -------------------------------------------
# Main RAG Pipeline
# -------------------------------------------
def studymate(pdf_files, question, hf_token):
    if not pdf_files:
        return "Upload at least one PDF.", "", ""

    start = time.time()

    pages = extract_pages(pdf_files)
    if not pages:
        return "No extractable text found in the PDFs.", "", ""

    chunks = chunk_text(pages)
    index = build_index(chunks)
    retrieved = retrieve(question, chunks, index)

    prompt = build_prompt(question, retrieved)
    answer = call_granite(prompt, hf_token)

    sources = "\n".join(
        [
            f"{i+1}. {c['file']} â€” page {c['page']} (score={s:.3f})"
            for i, (c, s) in enumerate(retrieved)
        ]
    )

    return answer, sources, f"{time.time() - start:.1f} sec"


# ---------------------------------

